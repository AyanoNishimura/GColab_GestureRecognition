{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GestureRecognition.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AyanoNishimura/GColab_GestureRecognition/blob/master/GestureRecognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "w_325UssPpPV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "80f2ce52-fb4f-4600-9a95-597953f2e334"
      },
      "cell_type": "code",
      "source": [
        "# Import Chainer \n",
        "from chainer import Chain, Variable, optimizers, serializers, datasets, training, cuda\n",
        "from chainer.training import extensions\n",
        "import chainer.functions as F\n",
        "import chainer.links as L\n",
        "from chainer import Variable\n",
        "import chainer\n",
        "\n",
        "# Import NumPy and CuPy\n",
        "import numpy as np\n",
        "# import cupy as cp\n",
        "\n",
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# mout drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "print('Chainer version: ', chainer.__version__)\n",
        "print('GPU availability:', chainer.cuda.available)\n",
        "print('cuDNN availablility:', chainer.cuda.cudnn_enabled)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "Chainer version:  5.0.0\n",
            "GPU availability: True\n",
            "cuDNN availablility: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NxOMD6U5h0vW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "モデル"
      ]
    },
    {
      "metadata": {
        "id": "Y5pDXqGeYo_C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ClassificationModel(chainer.Chain):\n",
        "    def __init__(self, n_units, n_out):\n",
        "        super(ClassificationModel, self).__init__()\n",
        "        with self.init_scope():\n",
        "            self.l1 = L.LSTM(None, n_units)\n",
        "            self.l2 = L.Linear(None, n_out)\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.l1.reset_state()\n",
        "\n",
        "    def forward(self, x_data, t_data, train=True):\n",
        "        x, t = Variable(cuda.to_gpu(x_data)), Variable(cuda.to_gpu(t_data))\n",
        "        h1 = self.l1(x)\n",
        "        y = self.l2(h1)\n",
        "        if train is False:\n",
        "            return F.softmax(y).data\n",
        "        return F.softmax_cross_entropy(y, t), F.accuracy(y, t)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SlMcbs_xh3q9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "学習用のクラス"
      ]
    },
    {
      "metadata": {
        "id": "xJHeZZKkYvIK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import chainer\n",
        "from chainer import optimizers\n",
        "from chainer import serializers\n",
        "import pandas as pd\n",
        "from progressbar import ProgressBar\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class TrainManager():\n",
        "    def __init__(self, is_debug):\n",
        "        self.is_debug = is_debug\n",
        "        self.n_units = 1000\n",
        "#         self.alpha = 10 ** np.random.uniform(-6, -2)\n",
        "#         self.weight_decay = 10 ** np.random.uniform(-8, -4)\n",
        "        self.alpha = 0.0000102454\n",
        "#         self.weight_decay = 0.0000025\n",
        "        self.weight_decay = 0\n",
        "        if is_debug:\n",
        "            self.batch_size = 100\n",
        "            self.epoch = 10\n",
        "        else:\n",
        "            self.batch_size = 1000\n",
        "            self.epoch = 500\n",
        "\n",
        "        self.models_path = '/gdrive/My Drive/DeepLearning/GestureRecognition/models/'\n",
        "        self.output_filename = 'epoch_' + str(self.epoch) + '_units_' + str(self.n_units) + '_lr_' + str(self.alpha) + '_wd_' + str(self.weight_decay)\n",
        "\n",
        "    ###\n",
        "    # split data train:test = 8:2, each gesture number\n",
        "    ###\n",
        "    def get_data(self, file_path):\n",
        "        data = pd.read_csv(file_path)\n",
        "        gesture_num = int(data['gesture'].max())\n",
        "        train = test = pd.DataFrame().reindex_like(data)[:0]\n",
        "        for index in range(0, gesture_num + 1):\n",
        "            tr, te = train_test_split(data.query('gesture == '+str(index)), test_size=0.2, shuffle=False)\n",
        "            train = train.append(tr)\n",
        "            test = test.append(te)\n",
        "        train = train.reset_index(drop=True)\n",
        "        test = test.reset_index(drop=True)\n",
        "\n",
        "        return train, test\n",
        "\n",
        "    ###\n",
        "    # batch Formatter\n",
        "    ###\n",
        "    def get_batch(self, x_batch, t_batch):\n",
        "        x_batch = x_batch.drop('participants', axis=1)\n",
        "        x_batch = x_batch.values.astype(np.float32)\n",
        "        t_batch = t_batch.drop('participants', axis=1)\n",
        "        t_batch = t_batch.values.astype(np.int32)\n",
        "        t_batch = t_batch.reshape(-1)\n",
        "\n",
        "        return x_batch, t_batch\n",
        "\n",
        "    ###\n",
        "    # training\n",
        "    ###\n",
        "    def training(self, x_train, t_train, x_test, t_test):\n",
        "        gesture_num = int(t_train['gesture'].max()) + 1\n",
        "        print('-'*5 +'gesture num'+ '-'*5)\n",
        "        print(gesture_num)\n",
        "        print('-'*5 +'Hyper Parameter'+ '-'*5)\n",
        "        print('lr: ', self.alpha)\n",
        "        print('weight decay: ', self.weight_decay)\n",
        "\n",
        "        # create onehot vector\n",
        "        t_vector = [i for i in range(0, gesture_num)]\n",
        "        n_labels = len(np.unique(t_vector))\n",
        "        t_vector = np.eye(n_labels)[t_vector]\n",
        "\n",
        "        model = ClassificationModel(self.n_units, gesture_num)\n",
        "        \n",
        "        # setting gpu\n",
        "        model.to_gpu()\n",
        "\n",
        "        optimizer = chainer.optimizers.Adam(alpha=self.alpha, weight_decay_rate=self.weight_decay)\n",
        "        # optimizer = chainer.optimizers.SGD(lr=0.01)\n",
        "        optimizer.setup(model)\n",
        "\n",
        "        train_loss, train_acc, train_acc_list = [], [], []\n",
        "        test_loss, test_acc, test_acc_list = [], [], []\n",
        "        print('-'*5 +'train start'+ '-'*5)\n",
        "        for epoch in range(self.epoch):\n",
        "            print('Epoch: %d' % (epoch+1))\n",
        "\n",
        "            train_sum_accuracy, train_sum_loss = 0, 0\n",
        "            prg = ProgressBar(0, len(x_train))\n",
        "            prg_num = 0\n",
        "            for current_participants in range(int(x_train['participants'].min()), int(x_train['participants'].max() + 1)):\n",
        "                x_data = x_train[x_train['participants'] == current_participants]\n",
        "                t_data = t_train[t_train['participants'] == current_participants]\n",
        "                x_data = x_data.reset_index(drop=True)\n",
        "                t_data = t_data.reset_index(drop=True)\n",
        "                model.reset_state()\n",
        "\n",
        "                # training data each batch size\n",
        "                for i in range(0, len(x_data), self.batch_size):\n",
        "                    x_batch = x_data.iloc[i:i+self.batch_size-1]\n",
        "                    t_batch = t_data.iloc[i:i+self.batch_size-1]\n",
        "                    x_batch, t_batch = self.get_batch(x_batch, t_batch)\n",
        "\n",
        "                    if self.is_debug:\n",
        "                        prg.update(prg_num)\n",
        "                        prg_num = prg_num + len(x_batch)\n",
        "\n",
        "                    # get loss and accuracy\n",
        "                    loss, acc = model.forward(x_batch, t_batch)\n",
        "                    # init gradient\n",
        "                    model.zerograds()\n",
        "                    loss.backward()\n",
        "                    loss.unchain_backward()\n",
        "\n",
        "                    optimizer.update()\n",
        "\n",
        "                    train_loss.append(loss.data)\n",
        "                    train_acc.append(acc.data)\n",
        "                    train_sum_loss += float(loss.data) * len(x_batch)\n",
        "                    train_sum_accuracy += float(acc.data) * len(x_batch)\n",
        "\n",
        "            # show training data loss and accuracy\n",
        "            print('train mean loss={}, accuracy={}'.format(train_sum_loss / len(x_train), train_sum_accuracy / len(x_train)))\n",
        "            train_acc_list.append(train_sum_accuracy / len(x_train))\n",
        "\n",
        "            # test data training\n",
        "            test_sum_accuracy, test_sum_loss = 0, 0\n",
        "            for current_participants in range(int(x_test['participants'].min()), int(x_test['participants'].max() + 1)):\n",
        "                x_data = x_test[x_test['participants'] == current_participants]\n",
        "                t_data = t_test[t_test['participants'] == current_participants]\n",
        "                x_data = x_data.reset_index(drop=True)\n",
        "                t_data = t_data.reset_index(drop=True)\n",
        "                model.reset_state()\n",
        "\n",
        "                # test data each batch size\n",
        "                for i in range(0, len(x_data), self.batch_size):\n",
        "                    x_batch = x_data.iloc[i:i+self.batch_size-1]\n",
        "                    t_batch = t_data.iloc[i:i+self.batch_size-1]\n",
        "                    x_batch, t_batch = self.get_batch(x_batch, t_batch)\n",
        "\n",
        "                    loss, acc = model.forward(x_batch, t_batch)\n",
        "\n",
        "                    test_loss.append(loss.data)\n",
        "                    test_acc.append(acc.data)\n",
        "                    test_sum_loss     += float(loss.data) * len(x_batch)\n",
        "                    test_sum_accuracy += float(acc.data) * len(x_batch)\n",
        "\n",
        "            print('test  mean loss={}, accuracy={}'.format(test_sum_loss / len(x_test), test_sum_accuracy / len(x_test)))\n",
        "            test_acc_list.append(test_sum_accuracy / len(x_test))\n",
        "\n",
        "        serializers.save_hdf5(self.models_path + self.output_filename, model)\n",
        "        return train_acc_list, test_acc_list\n",
        "\n",
        "    def classification_train(self, file_path):\n",
        "        # split data\n",
        "        train, test = self.get_data(file_path)\n",
        "        print('-'*5 +'data size'+ '-'*5)\n",
        "        print('train data: ', train.shape[0])\n",
        "        print('test data: ', test.shape[0])\n",
        "\n",
        "        t_train = pd.DataFrame({'gesture': train['gesture'], 'participants': train['participants']})\n",
        "        x_train = train.drop('gesture', axis=1)\n",
        "        t_test = pd.DataFrame({'gesture': test['gesture'], 'participants': test['participants']})\n",
        "        x_test = test.drop('gesture', axis=1)\n",
        "\n",
        "        train_acc_list, test_acc_list = self.training(x_train, t_train, x_test, t_test)\n",
        "\n",
        "        # show graph\n",
        "        plt.figure()\n",
        "        x = np.arange(len(train_acc_list))\n",
        "        plt.plot(x, train_acc_list, marker='o', label='train')\n",
        "        t = np.arange(len(test_acc_list))\n",
        "        plt.plot(t, test_acc_list, marker='+', label='test')\n",
        "        plt.xlabel('epochs')\n",
        "        plt.ylabel('accuracy')\n",
        "        plt.ylim(-0.05, 1.0)\n",
        "        plt.savefig(self.models_path + self.output_filename + '.png')\n",
        "        \n",
        "        print('Done!')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-XHppDcFh7lW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "実行部分"
      ]
    },
    {
      "metadata": {
        "id": "nmpHiDF4V13p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 15808
        },
        "outputId": "ddcd07d4-0679-4fd4-c6a0-2a5547df4207"
      },
      "cell_type": "code",
      "source": [
        "# run training\n",
        "is_debug = False\n",
        "if is_debug:\n",
        "  file_path = '/gdrive/My Drive/DeepLearning/GestureRecognition/data/small/train_classification.csv'\n",
        "else:\n",
        "  file_path = '/gdrive/My Drive/DeepLearning/GestureRecognition/data/formatted/train_classification.csv'\n",
        "\n",
        "tr = TrainManager(is_debug)\n",
        "tr.classification_train(file_path)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----data size-----\n",
            "train data:  1226407\n",
            "test data:  306607\n",
            "-----gesture num-----\n",
            "7\n",
            "-----Hyper Parameter-----\n",
            "lr:  1.02454e-05\n",
            "weight decay:  2.5e-06\n",
            "-----train start-----\n",
            "Epoch: 1\n",
            "train mean loss=1.9664088849502568, accuracy=0.1793906918078684\n",
            "test  mean loss=1.9199139921919788, accuracy=0.22259113477809866\n",
            "Epoch: 2\n",
            "train mean loss=1.81752433224253, accuracy=0.27991686279979044\n",
            "test  mean loss=1.946822404943876, accuracy=0.18287906001286078\n",
            "Epoch: 3\n",
            "train mean loss=1.8273637305414856, accuracy=0.2656475378418906\n",
            "test  mean loss=1.9605782153031324, accuracy=0.15031946419602626\n",
            "Epoch: 4\n",
            "train mean loss=1.8198555356419701, accuracy=0.2712639443177238\n",
            "test  mean loss=1.9535101862537592, accuracy=0.16687812113709252\n",
            "Epoch: 5\n",
            "train mean loss=1.8041909970981032, accuracy=0.2909181047554478\n",
            "test  mean loss=1.9375208727737794, accuracy=0.20151529471170104\n",
            "Epoch: 6\n",
            "train mean loss=1.791015936863874, accuracy=0.2914929548112767\n",
            "test  mean loss=1.933858944399917, accuracy=0.21820115008530033\n",
            "Epoch: 7\n",
            "train mean loss=1.7892451386407797, accuracy=0.28155008909068474\n",
            "test  mean loss=1.9283564555163926, accuracy=0.21494290753441866\n",
            "Epoch: 8\n",
            "train mean loss=1.7765083529983137, accuracy=0.30069626192495236\n",
            "test  mean loss=1.9136963077934055, accuracy=0.24165462604788676\n",
            "Epoch: 9\n",
            "train mean loss=1.7737355451211687, accuracy=0.29646438735364017\n",
            "test  mean loss=1.9281539866766797, accuracy=0.2380213113825573\n",
            "Epoch: 10\n",
            "train mean loss=1.763320912947207, accuracy=0.2976923650966085\n",
            "test  mean loss=1.925902468434605, accuracy=0.21319147935611824\n",
            "Epoch: 11\n",
            "train mean loss=1.7519321402998476, accuracy=0.29280084039073867\n",
            "test  mean loss=1.9325386373487723, accuracy=0.21141069818020555\n",
            "Epoch: 12\n",
            "train mean loss=1.7433656466878582, accuracy=0.30646433077870816\n",
            "test  mean loss=1.9340366893806438, accuracy=0.20981256109722618\n",
            "Epoch: 13\n",
            "train mean loss=1.74233954989529, accuracy=0.3091869178692973\n",
            "test  mean loss=1.9385959713124186, accuracy=0.23121455179965242\n",
            "Epoch: 14\n",
            "train mean loss=1.7347773278039211, accuracy=0.2977893961768643\n",
            "test  mean loss=1.924609768231519, accuracy=0.23111344490989075\n",
            "Epoch: 15\n",
            "train mean loss=1.7393965276493146, accuracy=0.2987001866230531\n",
            "test  mean loss=1.9265089540919482, accuracy=0.22300208463778612\n",
            "Epoch: 16\n",
            "train mean loss=1.7310376335185251, accuracy=0.30370668132584755\n",
            "test  mean loss=1.9245027419379703, accuracy=0.24214059120898132\n",
            "Epoch: 17\n",
            "train mean loss=1.715023988377476, accuracy=0.31146267088492546\n",
            "test  mean loss=1.9318062353042087, accuracy=0.1896825586564062\n",
            "Epoch: 18\n",
            "train mean loss=1.7083508050392062, accuracy=0.3018247611320721\n",
            "test  mean loss=1.9439668428440355, accuracy=0.1549181851901447\n",
            "Epoch: 19\n",
            "train mean loss=1.695387546998766, accuracy=0.31383137931736516\n",
            "test  mean loss=1.932017292329398, accuracy=0.18363572895394778\n",
            "Epoch: 20\n",
            "train mean loss=1.7130330215915441, accuracy=0.3072748278069121\n",
            "test  mean loss=1.9276942298612687, accuracy=0.13741369245036364\n",
            "Epoch: 21\n",
            "train mean loss=1.687319812816906, accuracy=0.3235247354912578\n",
            "test  mean loss=1.9380232297677482, accuracy=0.15825144251450166\n",
            "Epoch: 22\n",
            "train mean loss=1.6936850101579601, accuracy=0.3230656708396268\n",
            "test  mean loss=1.9429514327050328, accuracy=0.12526785094041457\n",
            "Epoch: 23\n",
            "train mean loss=1.691538320752756, accuracy=0.3188664124842602\n",
            "test  mean loss=1.947550398779324, accuracy=0.1304764731247748\n",
            "Epoch: 24\n",
            "train mean loss=1.6915496104238648, accuracy=0.32747366942825784\n",
            "test  mean loss=1.9417268745309941, accuracy=0.10320377578625586\n",
            "Epoch: 25\n",
            "train mean loss=1.6645744180446076, accuracy=0.3357091082130739\n",
            "test  mean loss=1.9390735343056504, accuracy=0.11563662947730073\n",
            "Epoch: 26\n",
            "train mean loss=1.6810285220041827, accuracy=0.3373023799982903\n",
            "test  mean loss=1.9436216050595545, accuracy=0.10866679445008404\n",
            "Epoch: 27\n",
            "train mean loss=1.6630524186263866, accuracy=0.3491915816624817\n",
            "test  mean loss=1.9516123729823098, accuracy=0.09984768791021624\n",
            "Epoch: 28\n",
            "train mean loss=1.6882826500302492, accuracy=0.34372194581926513\n",
            "test  mean loss=1.9157920415919645, accuracy=0.14364316530624976\n",
            "Epoch: 29\n",
            "train mean loss=1.6630415852146623, accuracy=0.37394763767554223\n",
            "test  mean loss=1.9189538720533499, accuracy=0.1338064685466485\n",
            "Epoch: 30\n",
            "train mean loss=1.666334385343524, accuracy=0.37816401876526917\n",
            "test  mean loss=1.9134729977366058, accuracy=0.14881591060399424\n",
            "Epoch: 31\n",
            "train mean loss=1.6558353090550897, accuracy=0.3826723103486948\n",
            "test  mean loss=1.9149712656361753, accuracy=0.11931234444638217\n",
            "Epoch: 32\n",
            "train mean loss=1.6588000156853788, accuracy=0.38127473201019035\n",
            "test  mean loss=1.9186067368976625, accuracy=0.13711037214940855\n",
            "Epoch: 33\n",
            "train mean loss=1.6482636376084183, accuracy=0.3932805341800028\n",
            "test  mean loss=1.907296982240356, accuracy=0.13912924355195924\n",
            "Epoch: 34\n",
            "train mean loss=1.6527094349113354, accuracy=0.3907128711857838\n",
            "test  mean loss=1.9193184984338518, accuracy=0.1328802012040878\n",
            "Epoch: 35\n",
            "train mean loss=1.6381218253122305, accuracy=0.3921406189185834\n",
            "test  mean loss=1.9280633029585739, accuracy=0.11491257491321204\n",
            "Epoch: 36\n",
            "train mean loss=1.6561120250068695, accuracy=0.3682496923815683\n",
            "test  mean loss=1.9063398432740881, accuracy=0.1417775851737572\n",
            "Epoch: 37\n",
            "train mean loss=1.6349363648135324, accuracy=0.3857895462095716\n",
            "test  mean loss=1.9079229937119109, accuracy=0.13049930292547965\n",
            "Epoch: 38\n",
            "train mean loss=1.6535212915062634, accuracy=0.38296421991651053\n",
            "test  mean loss=1.8798109350892118, accuracy=0.18826054182346721\n",
            "Epoch: 39\n",
            "train mean loss=1.6415939853960957, accuracy=0.4006793833438645\n",
            "test  mean loss=1.8965468735127111, accuracy=0.17447416432613877\n",
            "Epoch: 40\n",
            "train mean loss=1.6433296987578045, accuracy=0.4038471731131333\n",
            "test  mean loss=1.8864658289314133, accuracy=0.18764411834321057\n",
            "Epoch: 41\n",
            "train mean loss=1.6260712379372317, accuracy=0.4075221363548178\n",
            "test  mean loss=1.906607191946252, accuracy=0.16695313603459513\n",
            "Epoch: 42\n",
            "train mean loss=1.652919290981228, accuracy=0.3905383781661853\n",
            "test  mean loss=1.8886673594789234, accuracy=0.20844272968135005\n",
            "Epoch: 43\n",
            "train mean loss=1.6426751817365102, accuracy=0.4028417979020796\n",
            "test  mean loss=1.88739978973303, accuracy=0.1806286223402253\n",
            "Epoch: 44\n",
            "train mean loss=1.636296030158333, accuracy=0.40172226736913064\n",
            "test  mean loss=1.914409474038446, accuracy=0.15889721996895054\n",
            "Epoch: 45\n",
            "train mean loss=1.6312810899154735, accuracy=0.4078287226345919\n",
            "test  mean loss=1.9029525642903853, accuracy=0.15479424822921012\n",
            "Epoch: 46\n",
            "train mean loss=1.6388932002345726, accuracy=0.4097505971638475\n",
            "test  mean loss=1.8898516027848402, accuracy=0.17937620502807308\n",
            "Epoch: 47\n",
            "train mean loss=1.632368909062848, accuracy=0.41453612116080146\n",
            "test  mean loss=1.8911588285593954, accuracy=0.22226172255452253\n",
            "Epoch: 48\n",
            "train mean loss=1.6247266683987132, accuracy=0.4161530398134583\n",
            "test  mean loss=1.892451136122252, accuracy=0.21227499734542846\n",
            "Epoch: 49\n",
            "train mean loss=1.6376501163257497, accuracy=0.4054037531834631\n",
            "test  mean loss=1.922820539918763, accuracy=0.20999194417629857\n",
            "Epoch: 50\n",
            "train mean loss=1.6385384450572966, accuracy=0.41122237590782956\n",
            "test  mean loss=1.8915951176684438, accuracy=0.16636606430226347\n",
            "Epoch: 51\n",
            "train mean loss=1.623408865432268, accuracy=0.4102186306187541\n",
            "test  mean loss=1.8974791377879148, accuracy=0.1808438822456162\n",
            "Epoch: 52\n",
            "train mean loss=1.6278738370599533, accuracy=0.4017320514964678\n",
            "test  mean loss=1.8817781295104892, accuracy=0.22175292853526402\n",
            "Epoch: 53\n",
            "train mean loss=1.6258861771357105, accuracy=0.407942877277444\n",
            "test  mean loss=1.8735203475747904, accuracy=0.21905892491592538\n",
            "Epoch: 54\n",
            "train mean loss=1.6177891255894161, accuracy=0.40662113040388265\n",
            "test  mean loss=1.8754940481861526, accuracy=0.22603202125238323\n",
            "Epoch: 55\n",
            "train mean loss=1.6187498511070573, accuracy=0.41587580663145135\n",
            "test  mean loss=1.8832625334443727, accuracy=0.2086514663627495\n",
            "Epoch: 56\n",
            "train mean loss=1.615223549077014, accuracy=0.41426704270740444\n",
            "test  mean loss=1.8933180991322676, accuracy=0.21524948870403057\n",
            "Epoch: 57\n",
            "train mean loss=1.6109117484141546, accuracy=0.411413992944878\n",
            "test  mean loss=1.8896701175870416, accuracy=0.22315537489087256\n",
            "Epoch: 58\n",
            "train mean loss=1.5976786938616918, accuracy=0.4199030180535335\n",
            "test  mean loss=1.8894030249974907, accuracy=0.22046789539965764\n",
            "Epoch: 59\n",
            "train mean loss=1.5916685712807008, accuracy=0.41976032467745267\n",
            "test  mean loss=1.9351210421959526, accuracy=0.16037794364083707\n",
            "Epoch: 60\n",
            "train mean loss=1.6056236970577231, accuracy=0.411913826863928\n",
            "test  mean loss=1.909368141399647, accuracy=0.20889281723537986\n",
            "Epoch: 61\n",
            "train mean loss=1.604628468749626, accuracy=0.4130920655681288\n",
            "test  mean loss=1.900299118943589, accuracy=0.23628292910630572\n",
            "Epoch: 62\n",
            "train mean loss=1.6190407308972383, accuracy=0.41592636091306334\n",
            "test  mean loss=1.9266783222362442, accuracy=0.15680985726294308\n",
            "Epoch: 63\n",
            "train mean loss=1.613463026883212, accuracy=0.41120036073080823\n",
            "test  mean loss=1.9294401541708852, accuracy=0.18348896102525283\n",
            "Epoch: 64\n",
            "train mean loss=1.6006134925736955, accuracy=0.41374030036703735\n",
            "test  mean loss=1.9524844255379332, accuracy=0.15247531847082402\n",
            "Epoch: 65\n",
            "train mean loss=1.6313699228746665, accuracy=0.4031002764967104\n",
            "test  mean loss=1.8965652965950186, accuracy=0.1976406281853529\n",
            "Epoch: 66\n",
            "train mean loss=1.6021373997450554, accuracy=0.41916672071929817\n",
            "test  mean loss=1.9194880559276333, accuracy=0.20778716731470548\n",
            "Epoch: 67\n",
            "train mean loss=1.5878633159322821, accuracy=0.4301965014129909\n",
            "test  mean loss=1.9495188379543396, accuracy=0.15695010145536406\n",
            "Epoch: 68\n",
            "train mean loss=1.5880827557677113, accuracy=0.43553812105856393\n",
            "test  mean loss=1.956642729889429, accuracy=0.18026659531995853\n",
            "Epoch: 69\n",
            "train mean loss=1.6061683906263318, accuracy=0.4236774583008924\n",
            "test  mean loss=1.9402852563275343, accuracy=0.15789267703411966\n",
            "Epoch: 70\n",
            "train mean loss=1.600752970197468, accuracy=0.4179583129821213\n",
            "test  mean loss=1.9355669349298146, accuracy=0.17835535333202807\n",
            "Epoch: 71\n",
            "train mean loss=1.5854604911518504, accuracy=0.43204662120148707\n",
            "test  mean loss=1.9545648597780838, accuracy=0.1613466091818138\n",
            "Epoch: 72\n",
            "train mean loss=1.6015778192196333, accuracy=0.4162370245170319\n",
            "test  mean loss=1.9326442630174414, accuracy=0.20471809157028245\n",
            "Epoch: 73\n",
            "train mean loss=1.6207838048145904, accuracy=0.41139279239422494\n",
            "test  mean loss=1.9388687887417961, accuracy=0.19815920688805508\n",
            "Epoch: 74\n",
            "train mean loss=1.5927370273964387, accuracy=0.4221991563390501\n",
            "test  mean loss=1.933809124034671, accuracy=0.15880263675540704\n",
            "Epoch: 75\n",
            "train mean loss=1.5973583170328063, accuracy=0.4233178712012265\n",
            "test  mean loss=1.9110242238321395, accuracy=0.20098693071656323\n",
            "Epoch: 76\n",
            "train mean loss=1.593406027708865, accuracy=0.4184442849871984\n",
            "test  mean loss=1.9223677532875068, accuracy=0.17213892683493356\n",
            "Epoch: 77\n",
            "train mean loss=1.582213733847905, accuracy=0.4271355277207979\n",
            "test  mean loss=1.9391914955455918, accuracy=0.14991177597229138\n",
            "Epoch: 78\n",
            "train mean loss=1.5847291490619484, accuracy=0.4246127104881771\n",
            "test  mean loss=1.9338859545250133, accuracy=0.17252378454447095\n",
            "Epoch: 79\n",
            "train mean loss=1.5821088607119058, accuracy=0.41872885650507324\n",
            "test  mean loss=1.9640960254137994, accuracy=0.14777549119537778\n",
            "Epoch: 80\n",
            "train mean loss=1.582741213571123, accuracy=0.42233695689879636\n",
            "test  mean loss=1.904049328509995, accuracy=0.2106214155631899\n",
            "Epoch: 81\n",
            "train mean loss=1.5769900972758972, accuracy=0.42563847112106273\n",
            "test  mean loss=1.9185707681565678, accuracy=0.1908110376713394\n",
            "Epoch: 82\n",
            "train mean loss=1.5710556508526343, accuracy=0.4299910230161971\n",
            "test  mean loss=1.9451837040533257, accuracy=0.19055990310195955\n",
            "Epoch: 83\n",
            "train mean loss=1.5654294882032582, accuracy=0.42201569359208646\n",
            "test  mean loss=1.9192712819974747, accuracy=0.19079146944188585\n",
            "Epoch: 84\n",
            "train mean loss=1.5625980147851137, accuracy=0.42944634270363247\n",
            "test  mean loss=1.9582095677060556, accuracy=0.15190129400072067\n",
            "Epoch: 85\n",
            "train mean loss=1.5710241355222654, accuracy=0.4254664240672428\n",
            "test  mean loss=1.9297697936276235, accuracy=0.18563829280181088\n",
            "Epoch: 86\n",
            "train mean loss=1.5684358786993922, accuracy=0.4360216472911349\n",
            "test  mean loss=1.9321414626319993, accuracy=0.19896479836831832\n",
            "Epoch: 87\n",
            "train mean loss=1.559216565267479, accuracy=0.4320645596274637\n",
            "test  mean loss=1.9769156734384146, accuracy=0.17553415252538537\n",
            "Epoch: 88\n",
            "train mean loss=1.5670046825029107, accuracy=0.43146198650991396\n",
            "test  mean loss=1.953955256832636, accuracy=0.1688578533777571\n",
            "Epoch: 89\n",
            "train mean loss=1.5666484585260492, accuracy=0.4282509809471899\n",
            "test  mean loss=1.9521904589831587, accuracy=0.18586659720519352\n",
            "Epoch: 90\n",
            "train mean loss=1.5629567569670781, accuracy=0.4335608007330344\n",
            "test  mean loss=1.948974684677898, accuracy=0.21981559513959933\n",
            "Epoch: 91\n",
            "train mean loss=1.565486851578046, accuracy=0.4297953295107425\n",
            "test  mean loss=1.954839640278166, accuracy=0.1904881497631963\n",
            "Epoch: 92\n",
            "train mean loss=1.5614586971321656, accuracy=0.43299410460317567\n",
            "test  mean loss=1.9492241157113783, accuracy=0.18294429043487204\n",
            "Epoch: 93\n",
            "train mean loss=1.5689173161263272, accuracy=0.4156034669486068\n",
            "test  mean loss=1.959624382624533, accuracy=0.21999497679249436\n",
            "Epoch: 94\n",
            "train mean loss=1.5662097636061785, accuracy=0.4164196724485526\n",
            "test  mean loss=1.9454123434806077, accuracy=0.17564830577184695\n",
            "Epoch: 95\n",
            "train mean loss=1.5621633612775223, accuracy=0.41834480774706695\n",
            "test  mean loss=1.9918978193684649, accuracy=0.1798328151639843\n",
            "Epoch: 96\n",
            "train mean loss=1.5706092265886082, accuracy=0.42456786411754754\n",
            "test  mean loss=1.9482716471173074, accuracy=0.21302514352965282\n",
            "Epoch: 97\n",
            "train mean loss=1.5693201424476948, accuracy=0.4227829758457682\n",
            "test  mean loss=1.9805564990120064, accuracy=0.14677094755957978\n",
            "Epoch: 98\n",
            "train mean loss=1.5677790086384236, accuracy=0.43263533283197886\n",
            "test  mean loss=1.9769153210251529, accuracy=0.20542910015070626\n",
            "Epoch: 99\n",
            "train mean loss=1.5652534271747078, accuracy=0.4254493007745758\n",
            "test  mean loss=2.0392319197680107, accuracy=0.1282390809820557\n",
            "Epoch: 100\n",
            "train mean loss=1.5748288826821186, accuracy=0.43327052157947005\n",
            "test  mean loss=1.9921633366776292, accuracy=0.19208954826794375\n",
            "Epoch: 101\n",
            "train mean loss=1.570689002867543, accuracy=0.4301956861186721\n",
            "test  mean loss=2.045029520797563, accuracy=0.1273486908132286\n",
            "Epoch: 102\n",
            "train mean loss=1.5756079692405878, accuracy=0.4364440191961361\n",
            "test  mean loss=2.0294059997957064, accuracy=0.15210350733431324\n",
            "Epoch: 103\n",
            "train mean loss=1.5645075155754935, accuracy=0.4424183819146353\n",
            "test  mean loss=2.020107715594789, accuracy=0.128555447099824\n",
            "Epoch: 104\n",
            "train mean loss=1.5717733602532369, accuracy=0.43840666315690024\n",
            "test  mean loss=2.0341955028933385, accuracy=0.15303955853287146\n",
            "Epoch: 105\n",
            "train mean loss=1.5712641030023289, accuracy=0.44428480955021576\n",
            "test  mean loss=2.017288894694409, accuracy=0.1429386803588078\n",
            "Epoch: 106\n",
            "train mean loss=1.5595276475224669, accuracy=0.4472169519879171\n",
            "test  mean loss=2.0365545590452, accuracy=0.1490670468436346\n",
            "Epoch: 107\n",
            "train mean loss=1.5496004338328306, accuracy=0.4534334852916083\n",
            "test  mean loss=2.0162223056168944, accuracy=0.13319004499373607\n",
            "Epoch: 108\n",
            "train mean loss=1.536852839308261, accuracy=0.4644037427535873\n",
            "test  mean loss=2.041591601842417, accuracy=0.14473250700890344\n",
            "Epoch: 109\n",
            "train mean loss=1.5348979248766406, accuracy=0.46119925991371163\n",
            "test  mean loss=2.0760249699442443, accuracy=0.13085480796240417\n",
            "Epoch: 110\n",
            "train mean loss=1.545844289281616, accuracy=0.4585565808439401\n",
            "test  mean loss=2.059954213355869, accuracy=0.13435114033111112\n",
            "Epoch: 111\n",
            "train mean loss=1.5354658720996788, accuracy=0.46450566604400617\n",
            "test  mean loss=2.091696878579815, accuracy=0.13428917155272557\n",
            "Epoch: 112\n",
            "train mean loss=1.5509423810958862, accuracy=0.4536430408869057\n",
            "test  mean loss=2.091452761885553, accuracy=0.13313133746392913\n",
            "Epoch: 113\n",
            "train mean loss=1.5357739876390308, accuracy=0.45667792223425974\n",
            "test  mean loss=2.1188366541351, accuracy=0.12527437352442833\n",
            "Epoch: 114\n",
            "train mean loss=1.5487720776220382, accuracy=0.45231966247664285\n",
            "test  mean loss=2.0853668502003098, accuracy=0.13210396370122215\n",
            "Epoch: 115\n",
            "train mean loss=1.5319377041674762, accuracy=0.45782599154071957\n",
            "test  mean loss=2.11812432138036, accuracy=0.13393366783089206\n",
            "Epoch: 116\n",
            "train mean loss=1.5419618314743513, accuracy=0.4527616044197266\n",
            "test  mean loss=2.1283613044761633, accuracy=0.12859784728533435\n",
            "Epoch: 117\n",
            "train mean loss=1.537922031574097, accuracy=0.45270941928702835\n",
            "test  mean loss=2.106002136453106, accuracy=0.14458247912119856\n",
            "Epoch: 118\n",
            "train mean loss=1.5399595993624902, accuracy=0.45528034352467484\n",
            "test  mean loss=2.1184130034496578, accuracy=0.13089068487907118\n",
            "Epoch: 119\n",
            "train mean loss=1.5275467774106894, accuracy=0.4589650911152943\n",
            "test  mean loss=2.1287618659127676, accuracy=0.13302370810998243\n",
            "Epoch: 120\n",
            "train mean loss=1.5272993080835715, accuracy=0.4514317026691384\n",
            "test  mean loss=2.134925080816073, accuracy=0.13522848502680435\n",
            "Epoch: 121\n",
            "train mean loss=1.552374723775407, accuracy=0.43681094470794113\n",
            "test  mean loss=2.144011663079408, accuracy=0.12908707249860693\n",
            "Epoch: 122\n",
            "train mean loss=1.5438774120263685, accuracy=0.4332476904695752\n",
            "test  mean loss=2.10829441443922, accuracy=0.14809838011009402\n",
            "Epoch: 123\n",
            "train mean loss=1.5506359500592333, accuracy=0.44754229209042085\n",
            "test  mean loss=2.1153397776207776, accuracy=0.13183652020843906\n",
            "Epoch: 124\n",
            "train mean loss=1.525901418144703, accuracy=0.44657768648739465\n",
            "test  mean loss=2.120362739873104, accuracy=0.12990570977234428\n",
            "Epoch: 125\n",
            "train mean loss=1.5399817341503472, accuracy=0.441966655892221\n",
            "test  mean loss=2.142161571182907, accuracy=0.12920774777501\n",
            "Epoch: 126\n",
            "train mean loss=1.5408237109903846, accuracy=0.44407770060698754\n",
            "test  mean loss=2.111777878916829, accuracy=0.131598431292799\n",
            "Epoch: 127\n",
            "train mean loss=1.535411535723788, accuracy=0.44666656334087673\n",
            "test  mean loss=2.1385815307292058, accuracy=0.13328789008323852\n",
            "Epoch: 128\n",
            "train mean loss=1.5389529159339046, accuracy=0.4465320244944405\n",
            "test  mean loss=2.1376553586780904, accuracy=0.13472947468456478\n",
            "Epoch: 129\n",
            "train mean loss=1.5566898236005562, accuracy=0.4275154984144243\n",
            "test  mean loss=2.1725402362346875, accuracy=0.13251165240262636\n",
            "Epoch: 130\n",
            "train mean loss=1.5548617770607551, accuracy=0.440525046126934\n",
            "test  mean loss=2.1604368656907433, accuracy=0.13572749499419437\n",
            "Epoch: 131\n",
            "train mean loss=1.5699241485886626, accuracy=0.42437217064626387\n",
            "test  mean loss=2.158066890161271, accuracy=0.13194088794547496\n",
            "Epoch: 132\n",
            "train mean loss=1.5703142864837978, accuracy=0.4308789830746425\n",
            "test  mean loss=2.1367032973484377, accuracy=0.12447204395701233\n",
            "Epoch: 133\n",
            "train mean loss=1.592516639007808, accuracy=0.3982063054064697\n",
            "test  mean loss=2.124190171315225, accuracy=0.12631805540966612\n",
            "Epoch: 134\n",
            "train mean loss=1.5656804836589953, accuracy=0.43646766553943683\n",
            "test  mean loss=2.0967597304162524, accuracy=0.11788054433060524\n",
            "Epoch: 135\n",
            "train mean loss=1.5961968601969125, accuracy=0.41151591620343636\n",
            "test  mean loss=2.0961838134294513, accuracy=0.11753808612159182\n",
            "Epoch: 136\n",
            "train mean loss=1.6138244546150018, accuracy=0.4202128660096203\n",
            "test  mean loss=2.0562571958996525, accuracy=0.1194134513779886\n",
            "Epoch: 137\n",
            "train mean loss=1.6022751698584738, accuracy=0.4256515173073632\n",
            "test  mean loss=2.0965967640303225, accuracy=0.1157931814247287\n",
            "Epoch: 138\n",
            "train mean loss=1.6096044163328, accuracy=0.4164433180404375\n",
            "test  mean loss=2.0811866069997333, accuracy=0.11678794000562263\n",
            "Epoch: 139\n",
            "train mean loss=1.6368233327612896, accuracy=0.3833621304637772\n",
            "test  mean loss=2.025050793154837, accuracy=0.11812515671698297\n",
            "Epoch: 140\n",
            "train mean loss=1.6131338074465198, accuracy=0.40877783692142433\n",
            "test  mean loss=2.0442872190126162, accuracy=0.11810232680082154\n",
            "Epoch: 141\n",
            "train mean loss=1.6224556251605897, accuracy=0.3945949431873774\n",
            "test  mean loss=2.0048701343397046, accuracy=0.11676511018322505\n",
            "Epoch: 142\n",
            "train mean loss=1.6046646043036055, accuracy=0.4194031839371755\n",
            "test  mean loss=2.0146163442427727, accuracy=0.11523220203455169\n",
            "Epoch: 143\n",
            "train mean loss=1.5987109528132109, accuracy=0.40447828512199396\n",
            "test  mean loss=2.0283518062698858, accuracy=0.11639655950052054\n",
            "Epoch: 144\n",
            "train mean loss=1.615848949831794, accuracy=0.41206630448311166\n",
            "test  mean loss=1.993365905332992, accuracy=0.11783814415814517\n",
            "Epoch: 145\n",
            "train mean loss=1.6019529090014313, accuracy=0.4256670095921755\n",
            "test  mean loss=1.9957360912694437, accuracy=0.11628566852421755\n",
            "Epoch: 146\n",
            "train mean loss=1.6173566340441636, accuracy=0.42332031676513293\n",
            "test  mean loss=1.9966726980404623, accuracy=0.11954391082942938\n",
            "Epoch: 147\n",
            "train mean loss=1.5950315259073073, accuracy=0.42891063087136905\n",
            "test  mean loss=2.008512096102355, accuracy=0.11979830893277611\n",
            "Epoch: 148\n",
            "train mean loss=1.5955325785262295, accuracy=0.4272097274598325\n",
            "test  mean loss=2.023199594489505, accuracy=0.12243360382601087\n",
            "Epoch: 149\n",
            "train mean loss=1.6159944217146738, accuracy=0.4006826449038524\n",
            "test  mean loss=2.0152080052834576, accuracy=0.11689230824930043\n",
            "Epoch: 150\n",
            "train mean loss=1.609880730318282, accuracy=0.41271535496227435\n",
            "test  mean loss=1.9870530747182475, accuracy=0.11899271770975828\n",
            "Epoch: 151\n",
            "train mean loss=1.6061627146913178, accuracy=0.4188894879412591\n",
            "test  mean loss=1.997278409432873, accuracy=0.11866656675770194\n",
            "Epoch: 152\n",
            "train mean loss=1.6143642880015703, accuracy=0.42863665959587166\n",
            "test  mean loss=2.0154650056869188, accuracy=0.1199907377894949\n",
            "Epoch: 153\n",
            "train mean loss=1.5955365991439743, accuracy=0.4279354244419663\n",
            "test  mean loss=2.013916928507031, accuracy=0.11882964209363643\n",
            "Epoch: 154\n",
            "train mean loss=1.6057706978574768, accuracy=0.4206882384926431\n",
            "test  mean loss=2.0362759851683703, accuracy=0.12056150051992717\n",
            "Epoch: 155\n",
            "train mean loss=1.5941306844814522, accuracy=0.41977663260013054\n",
            "test  mean loss=2.0353246260015614, accuracy=0.11641939041474589\n",
            "Epoch: 156\n",
            "train mean loss=1.5859110594195132, accuracy=0.4285070127799679\n",
            "test  mean loss=2.0325888929581155, accuracy=0.11649114317786521\n",
            "Epoch: 157\n",
            "train mean loss=1.585490715192189, accuracy=0.42387152047719673\n",
            "test  mean loss=2.0646908642258865, accuracy=0.11414286033082995\n",
            "Epoch: 158\n",
            "train mean loss=1.5902538574531815, accuracy=0.43043785633572756\n",
            "test  mean loss=2.066415135267606, accuracy=0.11478537654296016\n",
            "Epoch: 159\n",
            "train mean loss=1.602333642718061, accuracy=0.42001880265872593\n",
            "test  mean loss=2.050376870538183, accuracy=0.11425701296051664\n",
            "Epoch: 160\n",
            "train mean loss=1.586726482291327, accuracy=0.4210527174486743\n",
            "test  mean loss=2.0608851522754597, accuracy=0.11378409510146001\n",
            "Epoch: 161\n",
            "train mean loss=1.5723922812152287, accuracy=0.43117170715237535\n",
            "test  mean loss=2.061351969788036, accuracy=0.10476929716579626\n",
            "Epoch: 162\n",
            "train mean loss=1.586103080139861, accuracy=0.4265875849717844\n",
            "test  mean loss=2.074112756725064, accuracy=0.1101833944832273\n",
            "Epoch: 163\n",
            "train mean loss=1.572315229029094, accuracy=0.4240182908720086\n",
            "test  mean loss=2.0655739307038092, accuracy=0.107636159135335\n",
            "Epoch: 164\n",
            "train mean loss=1.589345884024982, accuracy=0.434336236130041\n",
            "test  mean loss=2.067444461864592, accuracy=0.11435159645055008\n",
            "Epoch: 165\n",
            "train mean loss=1.5858018486134677, accuracy=0.41288658683424356\n",
            "test  mean loss=2.0597875067112654, accuracy=0.11264256865984647\n",
            "Epoch: 166\n",
            "train mean loss=1.584515800649683, accuracy=0.4298377296775624\n",
            "test  mean loss=2.0673168583467376, accuracy=0.1104443143644487\n",
            "Epoch: 167\n",
            "train mean loss=1.5610154521356951, accuracy=0.4392693452232068\n",
            "test  mean loss=2.0656392280047764, accuracy=0.103128760588251\n",
            "Epoch: 168\n",
            "train mean loss=1.5835970578758083, accuracy=0.4214832434596027\n",
            "test  mean loss=2.076196578861948, accuracy=0.11284478184944084\n",
            "Epoch: 169\n",
            "train mean loss=1.5982496488216047, accuracy=0.39235751312028627\n",
            "test  mean loss=2.0504839321778783, accuracy=0.10098921364913126\n",
            "Epoch: 170\n",
            "train mean loss=1.5892719861161901, accuracy=0.4334246300764993\n",
            "test  mean loss=2.0664544920916708, accuracy=0.10146539375074005\n",
            "Epoch: 171\n",
            "train mean loss=1.5725578881040037, accuracy=0.41386260863197577\n",
            "test  mean loss=2.073589590514906, accuracy=0.10457034518609208\n",
            "Epoch: 172\n",
            "train mean loss=1.5868368962713515, accuracy=0.4344911603037293\n",
            "test  mean loss=2.09278387734353, accuracy=0.10920494266962598\n",
            "Epoch: 173\n",
            "train mean loss=1.5763687216386126, accuracy=0.4272668048126625\n",
            "test  mean loss=2.0431986223583416, accuracy=0.11352643593812924\n",
            "Epoch: 174\n",
            "train mean loss=1.6059501219666192, accuracy=0.40234930164431404\n",
            "test  mean loss=2.058568098647819, accuracy=0.11390150868359139\n",
            "Epoch: 175\n",
            "train mean loss=1.5993516794476421, accuracy=0.39808807383630135\n",
            "test  mean loss=2.038613086672393, accuracy=0.10905165235742272\n",
            "Epoch: 176\n",
            "train mean loss=1.6115468740637977, accuracy=0.4029429060102517\n",
            "test  mean loss=2.0684370304538358, accuracy=0.11376778730375393\n",
            "Epoch: 177\n",
            "train mean loss=1.5991677083939204, accuracy=0.40255396443700375\n",
            "test  mean loss=2.0168758904214217, accuracy=0.11640960592441287\n",
            "Epoch: 178\n",
            "train mean loss=1.6119085157677264, accuracy=0.3947311131060576\n",
            "test  mean loss=2.0202433424162582, accuracy=0.11753156331341762\n",
            "Epoch: 179\n",
            "train mean loss=1.6222131142809406, accuracy=0.39038508428947793\n",
            "test  mean loss=1.9759223428375148, accuracy=0.12509825218208215\n",
            "Epoch: 180\n",
            "train mean loss=1.6126819296884212, accuracy=0.403609079919368\n",
            "test  mean loss=2.0080433754182363, accuracy=0.143806240347025\n",
            "Epoch: 181\n",
            "train mean loss=1.6028108475536353, accuracy=0.4076191674786845\n",
            "test  mean loss=2.0296512619521394, accuracy=0.11579318163076355\n",
            "Epoch: 182\n",
            "train mean loss=1.605456569833315, accuracy=0.41319888148509454\n",
            "test  mean loss=2.0059844231963773, accuracy=0.1364776405111771\n",
            "Epoch: 183\n",
            "train mean loss=1.6164186331953492, accuracy=0.4075735055177964\n",
            "test  mean loss=2.0197216201510586, accuracy=0.13004595427255375\n",
            "Epoch: 184\n",
            "train mean loss=1.6018678070157468, accuracy=0.4078466613635832\n",
            "test  mean loss=2.010720475776951, accuracy=0.1455674525897191\n",
            "Epoch: 185\n",
            "train mean loss=1.5996594670746707, accuracy=0.41709318412137114\n",
            "test  mean loss=2.023465593040755, accuracy=0.12467099609032162\n",
            "Epoch: 186\n",
            "train mean loss=1.5933305213503335, accuracy=0.4251402678223868\n",
            "test  mean loss=2.0206212861317088, accuracy=0.12449161260363936\n",
            "Epoch: 187\n",
            "train mean loss=1.5911008025090818, accuracy=0.4311407228820081\n",
            "test  mean loss=2.026029751037142, accuracy=0.11989289205445916\n",
            "Epoch: 188\n",
            "train mean loss=1.5939103338351674, accuracy=0.4155439431473571\n",
            "test  mean loss=2.00852009350012, accuracy=0.13202894877691118\n",
            "Epoch: 189\n",
            "train mean loss=1.5877257866558787, accuracy=0.4269259723003306\n",
            "test  mean loss=2.025239154805086, accuracy=0.11954717264505536\n",
            "Epoch: 190\n",
            "train mean loss=1.58839085204368, accuracy=0.4251223295191109\n",
            "test  mean loss=2.0021872644795247, accuracy=0.14489884461061908\n",
            "Epoch: 191\n",
            "train mean loss=1.5817220847883113, accuracy=0.4338910333192734\n",
            "test  mean loss=2.0308399603826937, accuracy=0.13068194729696653\n",
            "Epoch: 192\n",
            "train mean loss=1.5632974378839226, accuracy=0.4370881775624151\n",
            "test  mean loss=2.038507942062684, accuracy=0.1289924888178891\n",
            "Epoch: 193\n",
            "train mean loss=1.574188116030619, accuracy=0.4366519435882933\n",
            "test  mean loss=2.045297591371444, accuracy=0.1322213777502602\n",
            "Epoch: 194\n",
            "train mean loss=1.5752868655097971, accuracy=0.423957136493939\n",
            "test  mean loss=2.0236080141107426, accuracy=0.13111246695713022\n",
            "Epoch: 195\n",
            "train mean loss=1.5856384416321412, accuracy=0.43197731288896624\n",
            "test  mean loss=2.0271614191017804, accuracy=0.1390444446468671\n",
            "Epoch: 196\n",
            "train mean loss=1.5766002569794801, accuracy=0.42554551675765867\n",
            "test  mean loss=2.020272119018228, accuracy=0.1468231320442577\n",
            "Epoch: 197\n",
            "train mean loss=1.583130759752714, accuracy=0.43006848506031514\n",
            "test  mean loss=2.049672435115731, accuracy=0.120995280708675\n",
            "Epoch: 198\n",
            "train mean loss=1.587288292314668, accuracy=0.42265414405336\n",
            "test  mean loss=2.0382404806660523, accuracy=0.1344457232081981\n",
            "Epoch: 199\n",
            "train mean loss=1.577716774618954, accuracy=0.4337295863335306\n",
            "test  mean loss=2.016335476409488, accuracy=0.12471013359084107\n",
            "Epoch: 200\n",
            "train mean loss=1.5744401038729245, accuracy=0.4336937093103372\n",
            "test  mean loss=2.054041514337352, accuracy=0.12407740205037009\n",
            "Epoch: 201\n",
            "train mean loss=1.5879746829666683, accuracy=0.4243803244538493\n",
            "test  mean loss=2.063936907571441, accuracy=0.11760657827240444\n",
            "Epoch: 202\n",
            "train mean loss=1.5949111234654314, accuracy=0.43299491966894116\n",
            "test  mean loss=2.0594697546625644, accuracy=0.11753482449761692\n",
            "Epoch: 203\n",
            "train mean loss=1.6016106389886766, accuracy=0.43171557286010487\n",
            "test  mean loss=2.0495497832114546, accuracy=0.11027145538573867\n",
            "Epoch: 204\n",
            "train mean loss=1.6148304949886672, accuracy=0.4223597879275701\n",
            "test  mean loss=2.0830755947612145, accuracy=0.11750220961234596\n",
            "Epoch: 205\n",
            "train mean loss=1.640647548936951, accuracy=0.4087509287953157\n",
            "test  mean loss=2.0526029070441316, accuracy=0.12412958552306352\n",
            "Epoch: 206\n",
            "train mean loss=1.61742901006034, accuracy=0.4209964557647471\n",
            "test  mean loss=2.087083411626844, accuracy=0.1169184004569062\n",
            "Epoch: 207\n",
            "train mean loss=1.5941557584951085, accuracy=0.43385760240721266\n",
            "test  mean loss=2.0510867312647987, accuracy=0.11613563911465365\n",
            "Epoch: 208\n",
            "train mean loss=1.625952306987973, accuracy=0.4164751183355444\n",
            "test  mean loss=2.067357033898307, accuracy=0.11427658172519126\n",
            "Epoch: 209\n",
            "train mean loss=1.652528412528197, accuracy=0.40998379865928114\n",
            "test  mean loss=2.0358222765075533, accuracy=0.12617781100936576\n",
            "Epoch: 210\n",
            "train mean loss=1.637802789728115, accuracy=0.41631856331107575\n",
            "test  mean loss=2.0148511475745763, accuracy=0.11703255303519439\n",
            "Epoch: 211\n",
            "train mean loss=1.6319694295860958, accuracy=0.4230773314343223\n",
            "test  mean loss=2.029762735520616, accuracy=0.11962218720251644\n",
            "Epoch: 212\n",
            "train mean loss=1.6438686241771394, accuracy=0.42394164445362764\n",
            "test  mean loss=2.0020915588828396, accuracy=0.12184327159263429\n",
            "Epoch: 213\n",
            "train mean loss=1.631803027387739, accuracy=0.4305177649864595\n",
            "test  mean loss=2.0227120371283873, accuracy=0.12275975537000623\n",
            "Epoch: 214\n",
            "train mean loss=1.6286245225942557, accuracy=0.4295939281744479\n",
            "test  mean loss=2.026504456427129, accuracy=0.12398934116378668\n",
            "Epoch: 215\n",
            "train mean loss=1.6213342978555705, accuracy=0.4281392720396653\n",
            "test  mean loss=2.0167386759716166, accuracy=0.11660203455041587\n",
            "Epoch: 216\n",
            "train mean loss=1.6209111497342785, accuracy=0.4333121061465339\n",
            "test  mean loss=2.010867795469386, accuracy=0.11863068958440119\n",
            "Epoch: 217\n",
            "train mean loss=1.6259396953735321, accuracy=0.4281865647884606\n",
            "test  mean loss=2.0279767081083295, accuracy=0.1161910856522074\n",
            "Epoch: 218\n",
            "train mean loss=1.6287882898706025, accuracy=0.42832191957856697\n",
            "test  mean loss=2.0358518271127926, accuracy=0.11568555184058565\n",
            "Epoch: 219\n",
            "train mean loss=1.611519996270476, accuracy=0.44492570627089284\n",
            "test  mean loss=2.050775944221026, accuracy=0.12361100755705381\n",
            "Epoch: 220\n",
            "train mean loss=1.6363428342509925, accuracy=0.4310738607204169\n",
            "test  mean loss=2.0567031917536114, accuracy=0.12025818062704288\n",
            "Epoch: 221\n",
            "train mean loss=1.6337543063914928, accuracy=0.4337377398251469\n",
            "test  mean loss=2.050131589214803, accuracy=0.1142700592339663\n",
            "Epoch: 222\n",
            "train mean loss=1.6257553389617971, accuracy=0.4339831718797208\n",
            "test  mean loss=2.042673369365284, accuracy=0.11965154051811032\n",
            "Epoch: 223\n",
            "train mean loss=1.636052734296481, accuracy=0.4353057347154494\n",
            "test  mean loss=2.063320426298159, accuracy=0.11693796951735214\n",
            "Epoch: 224\n",
            "train mean loss=1.6571982079487955, accuracy=0.41893107282313125\n",
            "test  mean loss=2.0516710178174407, accuracy=0.11975264706888826\n",
            "Epoch: 225\n",
            "train mean loss=1.6235842841123143, accuracy=0.42129325787468763\n",
            "test  mean loss=2.068993854214463, accuracy=0.10921798904405015\n",
            "Epoch: 226\n",
            "train mean loss=1.637017492594542, accuracy=0.42293300704781805\n",
            "test  mean loss=2.0654228935817684, accuracy=0.11922754541629106\n",
            "Epoch: 227\n",
            "train mean loss=1.6726209435541008, accuracy=0.40367431032289325\n",
            "test  mean loss=2.0498901557765987, accuracy=0.1219476398759129\n",
            "Epoch: 228\n",
            "train mean loss=1.6556844546831018, accuracy=0.4196486159878513\n",
            "test  mean loss=2.02872589505702, accuracy=0.10693167488782211\n",
            "Epoch: 229\n",
            "train mean loss=1.637901774124941, accuracy=0.42717548117158693\n",
            "test  mean loss=2.0611339364200747, accuracy=0.11972981700089981\n",
            "Epoch: 230\n",
            "train mean loss=1.6193591802404312, accuracy=0.43403209583313124\n",
            "test  mean loss=2.034158986632317, accuracy=0.11876114998553124\n",
            "Epoch: 231\n",
            "train mean loss=1.625417472279762, accuracy=0.42609835071362406\n",
            "test  mean loss=2.0294733073323163, accuracy=0.11155322654000147\n",
            "Epoch: 232\n",
            "train mean loss=1.6499623273436734, accuracy=0.41573474415732975\n",
            "test  mean loss=1.995840644319864, accuracy=0.11433202708674625\n",
            "Epoch: 233\n",
            "train mean loss=1.6339276842708412, accuracy=0.40617592728556945\n",
            "test  mean loss=2.026912065476364, accuracy=0.11926668316331679\n",
            "Epoch: 234\n",
            "train mean loss=1.6595498545662692, accuracy=0.40201906906969154\n",
            "test  mean loss=2.025706938652136, accuracy=0.12032667206311022\n",
            "Epoch: 235\n",
            "train mean loss=1.6589777441544586, accuracy=0.4074218433605727\n",
            "test  mean loss=1.9990548970907265, accuracy=0.11874810458923543\n",
            "Epoch: 236\n",
            "train mean loss=1.668841188075157, accuracy=0.3983832452232073\n",
            "test  mean loss=2.0375712171725757, accuracy=0.11822952469980488\n",
            "Epoch: 237\n",
            "train mean loss=1.6570850117233058, accuracy=0.3949985614964418\n",
            "test  mean loss=2.005213174376728, accuracy=0.12823581969544326\n",
            "Epoch: 238\n",
            "train mean loss=1.67283603847985, accuracy=0.39432423401547106\n",
            "test  mean loss=1.9988694459990826, accuracy=0.13190501226530293\n",
            "Epoch: 239\n",
            "train mean loss=1.6580242229356015, accuracy=0.3988838944643873\n",
            "test  mean loss=1.9991155548116109, accuracy=0.11393086222156762\n",
            "Epoch: 240\n",
            "train mean loss=1.6764354364452942, accuracy=0.39592892111176864\n",
            "test  mean loss=2.010690708576371, accuracy=0.11725759664953342\n",
            "Epoch: 241\n",
            "train mean loss=1.6917094281432983, accuracy=0.3939956317850917\n",
            "test  mean loss=1.987629290968355, accuracy=0.12690512640303747\n",
            "Epoch: 242\n",
            "train mean loss=1.6630155254193906, accuracy=0.4108293583472561\n",
            "test  mean loss=2.0496591049067843, accuracy=0.11815451084713743\n",
            "Epoch: 243\n",
            "train mean loss=1.666333274184176, accuracy=0.40872157518180485\n",
            "test  mean loss=2.032571466475181, accuracy=0.12055823896361893\n",
            "Epoch: 244\n",
            "train mean loss=1.6593289925230703, accuracy=0.4042988997418516\n",
            "test  mean loss=2.0935227284702234, accuracy=0.12477862589348034\n",
            "Epoch: 245\n",
            "train mean loss=1.6639058646499614, accuracy=0.41201167368504943\n",
            "test  mean loss=2.049444434012793, accuracy=0.12286412318127932\n",
            "Epoch: 246\n",
            "train mean loss=1.6552348844769555, accuracy=0.41731089329711235\n",
            "test  mean loss=2.0054747120436343, accuracy=0.12414263154306202\n",
            "Epoch: 247\n",
            "train mean loss=1.6534279315711236, accuracy=0.4176704803196254\n",
            "test  mean loss=2.055203137174065, accuracy=0.12064629960431078\n",
            "Epoch: 248\n",
            "train mean loss=1.664915515646572, accuracy=0.41618402411805655\n",
            "test  mean loss=2.014012624828076, accuracy=0.11397326284808416\n",
            "Epoch: 249\n",
            "train mean loss=1.6544029118848949, accuracy=0.4190117968212094\n",
            "test  mean loss=2.019094026376, accuracy=0.11582905823704655\n",
            "Epoch: 250\n",
            "train mean loss=1.6565761003348074, accuracy=0.42197247771160773\n",
            "test  mean loss=2.0336777539535382, accuracy=0.11732934974763645\n",
            "Epoch: 251\n",
            "train mean loss=1.644536332276578, accuracy=0.42646853819582786\n",
            "test  mean loss=2.0215953779225946, accuracy=0.11370255714267107\n",
            "Epoch: 252\n",
            "train mean loss=1.6435111071513036, accuracy=0.4257175640740635\n",
            "test  mean loss=2.0338209883639897, accuracy=0.11068892787461035\n",
            "Epoch: 253\n",
            "train mean loss=1.6537998602166368, accuracy=0.41730844733740236\n",
            "test  mean loss=2.0053843446397286, accuracy=0.11391455497248566\n",
            "Epoch: 254\n",
            "train mean loss=1.6477732458072707, accuracy=0.42088311699633846\n",
            "test  mean loss=2.015742674456583, accuracy=0.11590733342033734\n",
            "Epoch: 255\n",
            "train mean loss=1.6478946601426625, accuracy=0.42169769129304757\n",
            "test  mean loss=2.0347106422199905, accuracy=0.11553552235519328\n",
            "Epoch: 256\n",
            "train mean loss=1.6512506949226484, accuracy=0.4243485244998281\n",
            "test  mean loss=2.0355165173674235, accuracy=0.11126947517073443\n",
            "Epoch: 257\n",
            "train mean loss=1.6409196540757864, accuracy=0.42988746872889566\n",
            "test  mean loss=2.0216010346482705, accuracy=0.11399609278421723\n",
            "Epoch: 258\n",
            "train mean loss=1.6472832275016935, accuracy=0.4193216451181338\n",
            "test  mean loss=2.0308969579877987, accuracy=0.11440704208944932\n",
            "Epoch: 259\n",
            "train mean loss=1.6480394734327022, accuracy=0.4236668581478877\n",
            "test  mean loss=2.0078388881994313, accuracy=0.11049649821828313\n",
            "Epoch: 260\n",
            "train mean loss=1.6407546644704332, accuracy=0.4275513761931431\n",
            "test  mean loss=2.0467589582667114, accuracy=0.11217617291283226\n",
            "Epoch: 261\n",
            "train mean loss=1.6384438867730935, accuracy=0.4268949875066459\n",
            "test  mean loss=1.9897797802644386, accuracy=0.11238817130594783\n",
            "Epoch: 262\n",
            "train mean loss=1.6367516966142113, accuracy=0.42716895819916556\n",
            "test  mean loss=2.0208199988632507, accuracy=0.10892771521928614\n",
            "Epoch: 263\n",
            "train mean loss=1.637916605900314, accuracy=0.42774299284426265\n",
            "test  mean loss=1.987568507271673, accuracy=0.11056172846957897\n",
            "Epoch: 264\n",
            "train mean loss=1.644443803300581, accuracy=0.4228579912554413\n",
            "test  mean loss=2.023261769979633, accuracy=0.10777314297491017\n",
            "Epoch: 265\n",
            "train mean loss=1.6362272539965474, accuracy=0.42732958971507445\n",
            "test  mean loss=2.0642459002429923, accuracy=0.11066935840340274\n",
            "Epoch: 266\n",
            "train mean loss=1.6779088036315273, accuracy=0.41795342066304647\n",
            "test  mean loss=1.9984205117244138, accuracy=0.11062369726006219\n",
            "Epoch: 267\n",
            "train mean loss=1.6501369709705371, accuracy=0.4246314644187433\n",
            "test  mean loss=2.0325862816000257, accuracy=0.115829058109247\n",
            "Epoch: 268\n",
            "train mean loss=1.6552213814624974, accuracy=0.42864725975255497\n",
            "test  mean loss=2.0052424317715234, accuracy=0.11578991976150572\n",
            "Epoch: 269\n",
            "train mean loss=1.6563238147134254, accuracy=0.4199747724211955\n",
            "test  mean loss=1.968757162347786, accuracy=0.1147299314407962\n",
            "Epoch: 270\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_spec\u001b[0;34m(name, path, target)\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: '_SixMetaPathImporter' object has no attribute 'find_spec'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-89-7d5aa0bea89c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_debug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-88-bba698ad4a49>\u001b[0m in \u001b[0;36mclassification_train\u001b[0;34m(self, file_path)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gesture'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mtrain_acc_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;31m# show graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-88-bba698ad4a49>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(self, x_train, t_train, x_test, t_test)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                     \u001b[0;31m# get loss and accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m                     \u001b[0;31m# init gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzerograds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-80-e685c331f606>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_data, t_data, train)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mh1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/chainer/link.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforward\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0mforward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;31m# Call forward_postprocess hook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/chainer/links/connection/lstm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0mlstm_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m         \u001b[0mh_rest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/chainer/link.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforward\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0mforward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;31m# Call forward_postprocess hook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/chainer/links/connection/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, n_batch_axes)\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0min_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_batch_axes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_batch_axes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/chainer/functions/connection/linear.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(x, W, b, n_batch_axes)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_batch_axes\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_shape\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/chainer/function_node.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstatic_forward_optimizations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;31m# Check for output array types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/chainer/functions/connection/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     99\u001b[0m                                    outputs=[y])\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_add_bias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretain_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# b is not retained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/chainer/graph_optimizations/static_graph_utilities.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m                         \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschedule_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/chainer/functions/connection/linear.py\u001b[0m in \u001b[0;36mstatic_add_bias\u001b[0;34m(self, inputs, outputs)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mcupy/core/core.pyx\u001b[0m in \u001b[0;36mcupy.core.core.ndarray.__iadd__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mcupy/core/_kernel.pyx\u001b[0m in \u001b[0;36mcupy.core._kernel.ufunc.__call__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_spec\u001b[0;34m(name, path, target)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_spec_legacy\u001b[0;34m(finder, name, path)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}