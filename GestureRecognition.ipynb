{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GestureRecognition.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AyanoNishimura/GColab_GestureRecognition/blob/master/GestureRecognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "w_325UssPpPV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "37902b44-df0a-4d6a-9595-0af60a433b0a"
      },
      "cell_type": "code",
      "source": [
        "# Import Chainer \n",
        "from chainer import Chain, Variable, optimizers, serializers, datasets, training, cuda\n",
        "from chainer.training import extensions\n",
        "import chainer.functions as F\n",
        "import chainer.links as L\n",
        "from chainer import Variable\n",
        "import chainer\n",
        "\n",
        "# Import NumPy and CuPy\n",
        "import numpy as np\n",
        "# import cupy as cp\n",
        "\n",
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# mout drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "print('Chainer version: ', chainer.__version__)\n",
        "print('GPU availability:', chainer.cuda.available)\n",
        "print('cuDNN availablility:', chainer.cuda.cudnn_enabled)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "Chainer version:  5.0.0\n",
            "GPU availability: True\n",
            "cuDNN availablility: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NxOMD6U5h0vW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "モデル"
      ]
    },
    {
      "metadata": {
        "id": "Y5pDXqGeYo_C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ClassificationModel(chainer.Chain):\n",
        "    def __init__(self, n_units, n_out):\n",
        "        super(ClassificationModel, self).__init__()\n",
        "        with self.init_scope():\n",
        "            self.l1 = L.LSTM(None, n_units)\n",
        "            self.l2 = L.Linear(None, n_out)\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.l1.reset_state()\n",
        "\n",
        "    def forward(self, x_data, t_data, train=True):\n",
        "        x, t = Variable(cuda.to_gpu(x_data)), Variable(cuda.to_gpu(t_data))\n",
        "        h1 = self.l1(x)\n",
        "        y = self.l2(h1)\n",
        "        if train is False:\n",
        "            return F.softmax(y).data\n",
        "        return F.softmax_cross_entropy(y, t), F.accuracy(y, t)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SlMcbs_xh3q9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "学習用のクラス"
      ]
    },
    {
      "metadata": {
        "id": "xJHeZZKkYvIK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import chainer\n",
        "from chainer import optimizers\n",
        "from chainer import serializers\n",
        "import pandas as pd\n",
        "from progressbar import ProgressBar\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class TrainManager():\n",
        "    def __init__(self, is_debug):\n",
        "        self.is_debug = is_debug\n",
        "        self.n_units = 1000\n",
        "#         self.alpha = 10 ** np.random.uniform(-6, -2)\n",
        "#         self.weight_decay = 10 ** np.random.uniform(-8, -4)\n",
        "        self.alpha = 0.0000102454\n",
        "#         self.weight_decay = 0.0000025\n",
        "        self.weight_decay = 0\n",
        "        if is_debug:\n",
        "            self.batch_size = 100\n",
        "            self.epoch = 10\n",
        "        else:\n",
        "            self.batch_size = 1000\n",
        "            self.epoch = 500\n",
        "\n",
        "        self.models_path = '/gdrive/My Drive/DeepLearning/GestureRecognition/models/'\n",
        "        self.output_filename = 'epoch_' + str(self.epoch) + '_units_' + str(self.n_units) + '_lr_' + str(self.alpha) + '_wd_' + str(self.weight_decay)\n",
        "\n",
        "    ###\n",
        "    # split data train:test = 8:2, each gesture number\n",
        "    ###\n",
        "    def get_data(self, file_path):\n",
        "        data = pd.read_csv(file_path)\n",
        "        gesture_num = int(data['gesture'].max())\n",
        "        train = test = pd.DataFrame().reindex_like(data)[:0]\n",
        "        for index in range(0, gesture_num + 1):\n",
        "            tr, te = train_test_split(data.query('gesture == '+str(index)), test_size=0.2, shuffle=False)\n",
        "            train = train.append(tr)\n",
        "            test = test.append(te)\n",
        "        train = train.reset_index(drop=True)\n",
        "        test = test.reset_index(drop=True)\n",
        "\n",
        "        return train, test\n",
        "\n",
        "    ###\n",
        "    # batch Formatter\n",
        "    ###\n",
        "    def get_batch(self, x_batch, t_batch):\n",
        "        x_batch = x_batch.drop('participants', axis=1)\n",
        "        x_batch = x_batch.values.astype(np.float32)\n",
        "        t_batch = t_batch.drop('participants', axis=1)\n",
        "        t_batch = t_batch.values.astype(np.int32)\n",
        "        t_batch = t_batch.reshape(-1)\n",
        "\n",
        "        return x_batch, t_batch\n",
        "\n",
        "    ###\n",
        "    # training\n",
        "    ###\n",
        "    def training(self, x_train, t_train, x_test, t_test):\n",
        "        gesture_num = int(t_train['gesture'].max()) + 1\n",
        "        print('-'*5 +'gesture num'+ '-'*5)\n",
        "        print(gesture_num)\n",
        "        print('-'*5 +'Hyper Parameter'+ '-'*5)\n",
        "        print('lr: ', self.alpha)\n",
        "        print('weight decay: ', self.weight_decay)\n",
        "\n",
        "        # create onehot vector\n",
        "        t_vector = [i for i in range(0, gesture_num)]\n",
        "        n_labels = len(np.unique(t_vector))\n",
        "        t_vector = np.eye(n_labels)[t_vector]\n",
        "\n",
        "        model = ClassificationModel(self.n_units, gesture_num)\n",
        "        \n",
        "        # setting gpu\n",
        "        model.to_gpu()\n",
        "\n",
        "        optimizer = chainer.optimizers.Adam(alpha=self.alpha, weight_decay_rate=self.weight_decay)\n",
        "        # optimizer = chainer.optimizers.SGD(lr=0.01)\n",
        "        optimizer.setup(model)\n",
        "\n",
        "        train_loss, train_acc, train_acc_list = [], [], []\n",
        "        test_loss, test_acc, test_acc_list = [], [], []\n",
        "        print('-'*5 +'train start'+ '-'*5)\n",
        "        for epoch in range(self.epoch):\n",
        "            print('Epoch: %d' % (epoch+1))\n",
        "\n",
        "            train_sum_accuracy, train_sum_loss = 0, 0\n",
        "            prg = ProgressBar(0, len(x_train))\n",
        "            prg_num = 0\n",
        "            for current_participants in range(int(x_train['participants'].min()), int(x_train['participants'].max() + 1)):\n",
        "                x_data = x_train[x_train['participants'] == current_participants]\n",
        "                t_data = t_train[t_train['participants'] == current_participants]\n",
        "                x_data = x_data.reset_index(drop=True)\n",
        "                t_data = t_data.reset_index(drop=True)\n",
        "                model.reset_state()\n",
        "\n",
        "                # training data each batch size\n",
        "                for i in range(0, len(x_data), self.batch_size):\n",
        "                    x_batch = x_data.iloc[i:i+self.batch_size-1]\n",
        "                    t_batch = t_data.iloc[i:i+self.batch_size-1]\n",
        "                    x_batch, t_batch = self.get_batch(x_batch, t_batch)\n",
        "\n",
        "                    prg.update(prg_num)\n",
        "                    prg_num = prg_num + len(x_batch)\n",
        "\n",
        "                    # get loss and accuracy\n",
        "                    loss, acc = model.forward(x_batch, t_batch)\n",
        "                    # init gradient\n",
        "                    model.zerograds()\n",
        "                    loss.backward()\n",
        "                    loss.unchain_backward()\n",
        "\n",
        "                    optimizer.update()\n",
        "\n",
        "                    train_loss.append(loss.data)\n",
        "                    train_acc.append(acc.data)\n",
        "                    train_sum_loss += float(loss.data) * len(x_batch)\n",
        "                    train_sum_accuracy += float(acc.data) * len(x_batch)\n",
        "\n",
        "            # show training data loss and accuracy\n",
        "            print('train mean loss={}, accuracy={}'.format(train_sum_loss / len(x_train), train_sum_accuracy / len(x_train)))\n",
        "            train_acc_list.append(train_sum_accuracy / len(x_train))\n",
        "\n",
        "            # test data training\n",
        "            test_sum_accuracy, test_sum_loss = 0, 0\n",
        "            for current_participants in range(int(x_test['participants'].min()), int(x_test['participants'].max() + 1)):\n",
        "                x_data = x_test[x_test['participants'] == current_participants]\n",
        "                t_data = t_test[t_test['participants'] == current_participants]\n",
        "                x_data = x_data.reset_index(drop=True)\n",
        "                t_data = t_data.reset_index(drop=True)\n",
        "                model.reset_state()\n",
        "\n",
        "                # test data each batch size\n",
        "                for i in range(0, len(x_data), self.batch_size):\n",
        "                    x_batch = x_data.iloc[i:i+self.batch_size-1]\n",
        "                    t_batch = t_data.iloc[i:i+self.batch_size-1]\n",
        "                    x_batch, t_batch = self.get_batch(x_batch, t_batch)\n",
        "\n",
        "                    loss, acc = model.forward(x_batch, t_batch)\n",
        "\n",
        "                    test_loss.append(loss.data)\n",
        "                    test_acc.append(acc.data)\n",
        "                    test_sum_loss     += float(loss.data) * len(x_batch)\n",
        "                    test_sum_accuracy += float(acc.data) * len(x_batch)\n",
        "\n",
        "            print('test  mean loss={}, accuracy={}'.format(test_sum_loss / len(x_test), test_sum_accuracy / len(x_test)))\n",
        "            test_acc_list.append(test_sum_accuracy / len(x_test))\n",
        "\n",
        "        serializers.save_hdf5(self.models_path + self.output_filename, model)\n",
        "        return train_acc_list, test_acc_list\n",
        "\n",
        "    def classification_train(self, file_path):\n",
        "        # split data\n",
        "        train, test = self.get_data(file_path)\n",
        "        print('-'*5 +'data size'+ '-'*5)\n",
        "        print('train data: ', train.shape[0])\n",
        "        print('test data: ', test.shape[0])\n",
        "\n",
        "        t_train = pd.DataFrame({'gesture': train['gesture'], 'participants': train['participants']})\n",
        "        x_train = train.drop('gesture', axis=1)\n",
        "        t_test = pd.DataFrame({'gesture': test['gesture'], 'participants': test['participants']})\n",
        "        x_test = test.drop('gesture', axis=1)\n",
        "\n",
        "        train_acc_list, test_acc_list = self.training(x_train, t_train, x_test, t_test)\n",
        "\n",
        "        # show graph\n",
        "        plt.figure()\n",
        "        x = np.arange(len(train_acc_list))\n",
        "        plt.plot(x, train_acc_list, marker='o', label='train')\n",
        "        t = np.arange(len(test_acc_list))\n",
        "        plt.plot(t, test_acc_list, marker='+', label='test')\n",
        "        plt.xlabel('epochs')\n",
        "        plt.ylabel('accuracy')\n",
        "        plt.ylim(-0.05, 1.0)\n",
        "        plt.savefig(self.models_path + self.output_filename + '.png')\n",
        "        \n",
        "        print('Done!')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-XHppDcFh7lW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "実行部分"
      ]
    },
    {
      "metadata": {
        "id": "nmpHiDF4V13p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# run training\n",
        "is_debug = False\n",
        "if is_debug:\n",
        "  file_path = '/gdrive/My Drive/DeepLearning/GestureRecognition/data/small/train_classification.csv'\n",
        "else:\n",
        "  file_path = '/gdrive/My Drive/DeepLearning/GestureRecognition/data/formatted/train_classification.csv'\n",
        "\n",
        "tr = TrainManager(is_debug)\n",
        "tr.classification_train(file_path)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}